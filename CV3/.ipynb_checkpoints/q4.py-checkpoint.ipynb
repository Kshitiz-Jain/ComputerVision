{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000 25000 25000 25000\n",
      "25000\n",
      "400000\n",
      "HOGS computed\n",
      "Clustering done\n",
      "Bags of words created\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy\n",
    "import numpy as np\n",
    "import copy\n",
    "from sklearn.cluster import KMeans\n",
    "import math\n",
    "import random\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from skimage.feature import local_binary_pattern\n",
    "\n",
    "def confmat(clf, data, label,total):\n",
    "    matrix=np.zeros((total,total))\n",
    "    pros=[]\n",
    "    for i in range(len(data)):\n",
    "        temp=[]\n",
    "        temp.append(data[i])\n",
    "        cl=clf.predict(temp)\n",
    "        pros.append(clf.predict_proba(temp))\n",
    "        matrix[label[i]-1][cl-1]=matrix[label[i]-1][cl-1]+1\n",
    "    cor=0\n",
    "    for i in range(total):\n",
    "        cor=cor+matrix[i][i]\n",
    "    print(\"Accuracy :\",np.sum(matrix.diagonal())/np.sum(matrix))\n",
    "    return matrix,pros\n",
    "\n",
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "\n",
    "def data2():\n",
    "    traind=[]\n",
    "    trainl=[]\n",
    "    testd=[]\n",
    "    testl=[]\n",
    "    for i in range(5):\n",
    "        d=unpickle('./cifar-10-batches-py/data_batch_'+str(i+1))\n",
    "        data=d[b'data']\n",
    "        lbl=d[b'labels']\n",
    "        for i in range(len(data)):\n",
    "            img=cv2.cvtColor(np.reshape(data[i],(32,32,3)),cv2.COLOR_BGR2GRAY)\n",
    "            # img=np.ravel(img)\n",
    "            traind.append(img)\n",
    "            trainl.append(lbl[i])\n",
    "    d2=unpickle('./cifar-10-batches-py/test_batch')\n",
    "    data2=d2[b'data']\n",
    "    lbl2=d2[b'labels']\n",
    "    for i in range(len(data2)):\n",
    "        img=cv2.cvtColor(np.reshape(data2[i],(32,32,3)),cv2.COLOR_BGR2GRAY)\n",
    "        # img=np.ravel(img)\n",
    "        testd.append(img)\n",
    "        testl.append(lbl2[i])\n",
    "    return traind,trainl,testd,testl,(32,32)\n",
    "\n",
    "def distribute(data,label):\n",
    "    datacp=copy.deepcopy(data)\n",
    "    labelcp=copy.deepcopy(label)\n",
    "    a=int(len(labelcp)*0.5)\n",
    "    testdata=[]\n",
    "    testlabel=[]\n",
    "    length=len(labelcp)\n",
    "    for i in range(a):\n",
    "        ind=int(random.random()*(len(labelcp)-1))\n",
    "        testdata.append(datacp.pop(ind))\n",
    "        testlabel.append(labelcp.pop(ind))\n",
    "    print(len(datacp),len(labelcp),len(testdata),len(testlabel))\n",
    "    return datacp,labelcp,testdata,testlabel\n",
    "\n",
    "a,b,c,d,shape=data2()\n",
    "\n",
    "a1,b1,a2,b2=distribute(a,b)\n",
    "\n",
    "def hogdes():\n",
    "\twinSize = (8,8)\n",
    "\tblockSize = (4,4)\n",
    "\tblockStride = (2,2)\n",
    "\tcellSize = (2,2)\n",
    "\tnbins = 9\n",
    "\tderivAperture = 1\n",
    "\twinSigma = 4.0\n",
    "\thistogramNormType = 0\n",
    "\tL2HysThreshold = 0.20000000000000001\n",
    "\tgammaCorrection = 0\n",
    "\tnlevels = 8\n",
    "\tcalc_HOG =\tcv2.HOGDescriptor(winSize,blockSize,blockStride,cellSize,nbins,derivAperture,winSigma,histogramNormType,L2HysThreshold,gammaCorrection,nlevels)\n",
    "\treturn calc_HOG\n",
    "\n",
    "def divide(img,size):\n",
    "\trow,col=np.shape(img)\n",
    "\tpatches=[]\n",
    "\tfor i in range(0,row,size):\n",
    "\t\tfor j in range(0,col,size):\n",
    "\t\t\tpart=img[i:i+size,j:j+size]\n",
    "\t\t\tpatches.append(copy.deepcopy(part))\n",
    "\treturn patches\n",
    "\n",
    "# def make_lbphist(train_x, size):\n",
    "#   histogram = []\n",
    "#   for i in range(len(train_x)):\n",
    "#     for r in range(0,train_x[i].shape[0] - size, size):\n",
    "#       for c in range(0,train_x[i].shape[1] - size, size):\n",
    "#           patch = train_x[i][r : r+size, c : c+size]\n",
    "#           lbp = local_binary_pattern(patch, 8*3, 3, 'uniform')\n",
    "#         #   print (lbp.shape)\n",
    "#           (hist, _) = np.histogram(lbp.ravel(), bins=np.arange(0, 8*3 + 3), range=(0, 8*3 + 2))\n",
    "\n",
    "#             # normalize the histogram\n",
    "#           hist = hist.astype(\"float\")\n",
    "#           eps=1e-7\n",
    "#           hist /= (hist.sum() + eps)\n",
    "#           histogram.append(hist)\n",
    "#   return histogram\n",
    "\n",
    "def computehogs(data,size):\n",
    "    hogf=[]\n",
    "    hog=hogdes()\n",
    "    # hog=cv2.HOGDescriptor()\n",
    "    for i in range(len(data)):\n",
    "        patches=divide(data[i],size)\n",
    "        for j in range(len(patches)):\n",
    "            hf=hog.compute(patches[j])\n",
    "            hogf.append(hf.reshape((np.shape(hf)[0],)))\n",
    "#         print(i)\n",
    "    print(len(data))\n",
    "    print(len(hogf))\n",
    "    return hogf\n",
    "\n",
    "def computelbps(data,size):\n",
    "    lbpf=[]\n",
    "    for i in range(len(data)):\n",
    "        patches=divide(data[i],size)\n",
    "        for j in range(len(patches)):\n",
    "#             hf=hog.compute(patches[j])\n",
    "            lbp = local_binary_pattern(patches[j], 8*3, 3, 'uniform')\n",
    "            lbp.ravel()\n",
    "            (hist, _) = np.histogram(lbp, bins=np.arange(0, 8*3 + 3), range=(0, 8*3 + 2))\n",
    "            e=1e-7\n",
    "            hist=hist.astype(\"float\")\n",
    "            hist /= (hist.sum() + e)\n",
    "            lbpf.append(hist)\n",
    "#             hogf.append(hf.reshape((np.shape(hf)[0],)))\n",
    "#         print(i)\n",
    "    print(len(data))\n",
    "    print(len(lbpf))\n",
    "    return lbpf\n",
    "\n",
    "# words=computehogs(a1,8)\n",
    "# print(\"HOGS computed\")\n",
    "words=computelbps(a1,8)\n",
    "print(\"LBPS computed\")\n",
    "kmeans = KMeans(n_clusters=10, random_state=0).fit(np.array(words))\n",
    "print(\"Clustering done\")\n",
    "bow=kmeans.cluster_centers_\n",
    "print(\"Bags of words created\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minfrmcen(centers,dpt):\n",
    "\tind=0\n",
    "\tmaxi=9999\n",
    "\tfor i in range(len(centers)):\n",
    "\t\t# print(np.shape(dpt),np.shape(centers[i]))\n",
    "\t\tdist=np.linalg.norm(centers[i]-dpt,axis=0)\n",
    "\t\tif(dist<maxi):\n",
    "\t\t\tind=i\n",
    "\t\t\tmaxi=dist\n",
    "\treturn ind\n",
    "\n",
    "def cnvrttrn(a,bow,size):\n",
    "    data=[]\n",
    "# \trow=np.shape(a[0])\n",
    "    num=int(math.pow(32/size,2))\n",
    "    for i in range(0,len(a),num):\n",
    "        temp=np.zeros((np.shape(bow)[0]))\n",
    "        for j in range(i,i+num):\n",
    "            ind=minfrmcen(bow,a[j])\n",
    "            temp[ind]=temp[ind]+1\n",
    "        data.append(temp)\n",
    "#         print(i)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n",
      "400000\n"
     ]
    }
   ],
   "source": [
    "# newa2=computehogs(a2,8)\n",
    "newa2=computelbps(a2,8)\n",
    "trainx=cnvrttrn(newa2,bow,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "160000\n"
     ]
    }
   ],
   "source": [
    "# newc=computehogs(c,8)\n",
    "newc=computelbps(c,8)\n",
    "testx=cnvrttrn(newc,bow,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1761"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = GaussianNB()\n",
    "clf.fit(trainx,b2)\n",
    "clf.score(testx,d)\n",
    "# confmat(clf,testx,d,10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
