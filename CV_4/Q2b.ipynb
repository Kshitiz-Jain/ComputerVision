{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "\n",
    "class ConvNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(ConvNetwork,self).__init__()\n",
    "        #[in_channels,out_channels,kernel,stride,padding]\n",
    "        layer1=[3,16,3,1,1]\n",
    "        layer2=[16,32,3,1,1]\n",
    "        layer3=[32,64,3,1,1]\n",
    "        self.conv1=torch.nn.Conv2d(layer1[0],layer1[1],kernel_size=layer1[2],stride=layer1[3],padding=layer1[4])\n",
    "        self.conv2=torch.nn.Conv2d(layer2[0],layer2[1],kernel_size=layer2[2],stride=layer2[3],padding=layer2[4])\n",
    "        self.conv3=torch.nn.Conv2d(layer3[0],layer3[1],kernel_size=layer3[2],stride=layer3[3],padding=layer3[4])\n",
    "        self.fc1=nn.Linear(64*32*32,100)\n",
    "        self.fc2=torch.nn.Linear(100,50)\n",
    "        self.fc3=torch.nn.Linear(50,4)\n",
    "    \n",
    "        \n",
    "    def forward(self,inp):\n",
    "        out1=F.relu(self.conv1(inp))\n",
    "        out2=F.relu(self.conv2(out1))\n",
    "        out3=F.relu(self.conv3(out2))\n",
    "        out=out3.view(-1,out3.shape[1]*out3.shape[2]*out3.shape[3])\n",
    "        outfc1=self.fc1(out)\n",
    "        outfc2=self.fc2(outfc1)\n",
    "        outfc3=self.fc3(outfc2)\n",
    "        outf=F.softmax(outfc3,_stacklevel=4)\n",
    "        return outf\n",
    "\n",
    "model = ConvNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "\n",
    "\n",
    "def data2():\n",
    "    traind=[]\n",
    "    trainl=[]\n",
    "    testd=[]\n",
    "    testl=[]\n",
    "    keys={1:0,3:1,5:2,9:3}\n",
    "    for i in range(5):\n",
    "        d=unpickle('./data/cifar-10-batches-py/data_batch_'+str(i+1))\n",
    "        data=d[b'data']\n",
    "        lbl=d[b'labels']\n",
    "        for i in range(len(data)):\n",
    "            if(lbl[i] in keys):\n",
    "                img=np.reshape(data[i],(32,32,3))\n",
    "                nimg=[]\n",
    "                i1=img[:,:,0]\n",
    "                i2=img[:,:,1]\n",
    "                i3=img[:,:,2]\n",
    "                nimg.append(i1)\n",
    "                nimg.append(i2)\n",
    "                nimg.append(i3)\n",
    "                traind.append(nimg)\n",
    "                trainl.append(keys[lbl[i]])\n",
    "    d2=unpickle('./data/cifar-10-batches-py/test_batch')\n",
    "    data2=d2[b'data']\n",
    "    lbl2=d2[b'labels']\n",
    "    for i in range(len(data2)):\n",
    "        if(lbl2[i] in keys):\n",
    "            img=np.reshape(data2[i],(32,32,3))\n",
    "            nimg=[]\n",
    "            i1=img[:,:,0]\n",
    "            i2=img[:,:,1]\n",
    "            i3=img[:,:,2]\n",
    "            nimg.append(i1)\n",
    "            nimg.append(i2)\n",
    "            nimg.append(i3)\n",
    "            testd.append(nimg)\n",
    "            testl.append(keys[lbl2[i]])\n",
    "        \n",
    "#     for i in range(len(data2)):\n",
    "        \n",
    "#         img=cv2.cvtColor(np.reshape(data2[i],(32,32,3)),cv2.COLOR_BGR2GRAY)\n",
    "#         img=np.ravel(img)\n",
    "#         testd.append(img)\n",
    "#         testl.append(lbl2[i])\n",
    "    a=np.asarray(traind)\n",
    "    b=np.asarray(trainl)\n",
    "    c=np.asarray(testd)\n",
    "    d=np.asarray(testl)\n",
    "    return a,b,c,d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "m=[0.485, 0.456, 0.406]\n",
    "s=[0.229, 0.224, 0.225]\n",
    "transform=transforms.Compose([transforms.ToTensor(),transforms.Normalize(mean=m, std=s)])\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,download=True, transform=transform)\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=[]\n",
    "for x,y in (trainset):\n",
    "    if(y==1):\n",
    "        test.append((x,y))\n",
    "    elif(y==3):\n",
    "        test.append((x,y))\n",
    "    elif(y==5):\n",
    "        test.append((x,y))\n",
    "    elif(y==9):\n",
    "        test.append((x,y))\n",
    "\n",
    "train=[]\n",
    "for x,y in (trainset):\n",
    "    if(y==1):\n",
    "        train.append((x,y))\n",
    "    elif(y==3):\n",
    "        train.append((x,y))\n",
    "    elif(y==5):\n",
    "        train.append((x,y))\n",
    "    elif(y==9):\n",
    "        train.append((x,y))\n",
    "\n",
    "# print(np.shape(train),np.shape(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[batchsize,learning_rate,epochs,classes,momentum,worker]\n",
    "p=[100,0.001,3,4,0.9,2]\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(train, batch_size=p[0],shuffle=True, num_workers=p[5])\n",
    "testloader = torch.utils.data.DataLoader(test, batch_size=p[0],shuffle=False, num_workers=p[5])\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=p[1],momentum=p[4])\n",
    "\n",
    "\n",
    "for i in range(p[2]):\n",
    "    curr=0\n",
    "    l=0.0\n",
    "    for j,data in enumerate(trainloader,0):\n",
    "        x,y=data\n",
    "        y[y==1] = torch.from_numpy(np.zeros((y[y==1].size())).astype(np.long))\n",
    "        y[y==3] = torch.from_numpy(np.zeros((y[y==3].size())).astype(np.long)+1)\n",
    "        y[y==5] = torch.from_numpy(np.zeros((y[y==5].size())).astype(np.long)+2)\n",
    "        y[y==9] = torch.from_numpy(np.zeros((y[y==9].size())).astype(np.long)+3)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(x)\n",
    "        loss = criterion(outputs, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        l += loss.item()\n",
    "        if i % 100 == 99:\n",
    "          print('[%d, %5d] loss: %.3f' %(epoch + 1, i + 1, running_loss / 100))\n",
    "          l = 0.0\n",
    "        \n",
    "#     for j in range(batches):\n",
    "#         trnx=[]\n",
    "#         trny=[]\n",
    "#         for k in range(bsize):\n",
    "#             trnx.append(train_x[k+curr])\n",
    "#             trny.append(train_y[k+curr])\n",
    "#         curr=curr+bsize\n",
    "#         trnx=np.array(trnx).astype(np.float32)\n",
    "#         trny=np.array(trny).astype(np.long)\n",
    "#         trnx=torch.from_numpy(trnx)\n",
    "#         trny=torch.from_numpy(trny)\n",
    "\n",
    "#         outputs = model(trnx)\n",
    "#         optimizer.zero_grad()\n",
    "#         loss = criterion(outputs, trny)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         if(j%100==0):\n",
    "#             print(j,i,loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for data in test loader:\n",
    "        \n",
    "for i in range(p[2]):\n",
    "    curr=0\n",
    "    l=0.0\n",
    "    for j,data in enumerate(trainloader,0):\n",
    "        x,y=data\n",
    "        y[y==1] = torch.from_numpy(np.zeros((y[y==1].size())).astype(np.long))\n",
    "        y[y==3] = torch.from_numpy(np.zeros((y[y==3].size())).astype(np.long)+1)\n",
    "        y[y==5] = torch.from_numpy(np.zeros((y[y==5].size())).astype(np.long)+2)\n",
    "        y[y==9] = torch.from_numpy(np.zeros((y[y==9].size())).astype(np.long)+3)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(x)\n",
    "        loss = criterion(outputs, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        l += loss.item()\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    accurate=0\n",
    "    total=0\n",
    "    y_pred=[]\n",
    "    curr=0\n",
    "    for i in range(200):\n",
    "        tstx=[]\n",
    "        tsty=[]\n",
    "        for j in range(bsize):\n",
    "            tstx.append(test_x[j+curr])\n",
    "            tsty.append(test_y[j+curr])\n",
    "        curr=curr+bsize\n",
    "        tstx=np.array(tstx).astype(np.float32)\n",
    "        tsty=np.array(tsty).astype(np.long)\n",
    "        \n",
    "        print(np.shape(tstx))\n",
    "\n",
    "        tstx=torch.from_numpy(tstx)\n",
    "        tsty=torch.from_numpy(tsty)\n",
    "        \n",
    "        output=model(tstx)\n",
    "        _, predicted=torch.max(output.data,1)\n",
    "        y_pred.extend(predicted.numpy().tolist())\n",
    "        total=total+tsty.size(0)\n",
    "        accurate=accurate+(predicted==tsty).sum().item()\n",
    "    print(np.shape(y_pred))\n",
    "#     y_pred=clf.predict(test_x)\n",
    "    cm = confusion_matrix(test_y, y_pred)\n",
    "    print(np.sum(np.diagonal(cm))/np.sum(cm))\n",
    "    plt.clf()\n",
    "    plt.matshow(cm)\n",
    "    plt.colorbar()\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.savefig(\"ConfusionLR\")\n",
    "    print(\"Accuracy :\",accurate/total)\n",
    "\n",
    "\n",
    "train_x=torch.from_numpy(train_x)\n",
    "train_y=torch.from_numpy(train_y)\n",
    "test_x=torch.from_numpy(test_x)\n",
    "test_y=torch.from_numpy(test_y)\n",
    "print(train_x.size(),train_y.size(),test_x.size(),test_y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [100/500], Loss: 2.2286\n",
      "Epoch [1/5], Step [200/500], Loss: 2.1884\n",
      "Epoch [1/5], Step [300/500], Loss: 2.1278\n",
      "Epoch [1/5], Step [400/500], Loss: 2.0862\n",
      "Epoch [1/5], Step [500/500], Loss: 2.2566\n",
      "Epoch [2/5], Step [100/500], Loss: 2.2720\n",
      "Epoch [2/5], Step [200/500], Loss: 2.2945\n",
      "Epoch [2/5], Step [300/500], Loss: 2.2667\n",
      "Epoch [2/5], Step [400/500], Loss: 2.2271\n",
      "Epoch [2/5], Step [500/500], Loss: 2.2667\n",
      "Epoch [3/5], Step [100/500], Loss: 2.1516\n",
      "Epoch [3/5], Step [200/500], Loss: 2.1612\n",
      "Epoch [3/5], Step [300/500], Loss: 2.1846\n",
      "Epoch [3/5], Step [400/500], Loss: 2.1980\n",
      "Epoch [3/5], Step [500/500], Loss: 2.2660\n",
      "Epoch [4/5], Step [100/500], Loss: 2.2612\n",
      "Epoch [4/5], Step [200/500], Loss: 2.2220\n",
      "Epoch [4/5], Step [300/500], Loss: 2.2469\n",
      "Epoch [4/5], Step [400/500], Loss: 2.2612\n",
      "Epoch [4/5], Step [500/500], Loss: 2.2945\n",
      "Epoch [5/5], Step [100/500], Loss: 2.2339\n",
      "Epoch [5/5], Step [200/500], Loss: 2.3260\n",
      "Epoch [5/5], Step [300/500], Loss: 2.2751\n",
      "Epoch [5/5], Step [400/500], Loss: 2.2339\n",
      "Epoch [5/5], Step [500/500], Loss: 2.1754\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "#[batchsize,learning_rate,epochs,classes]\n",
    "param = [100,0.001,5,4]\n",
    "trainset = torch.utils.data.DataLoader(dataset=traindata,batch_size=param[0],shuffle=True)\n",
    "testset = torch.utils.data.DataLoader(dataset=testdata,batch_size=param[0],shuffle=False)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=param[1])\n",
    "\n",
    "\n",
    "# Train the model\n",
    "total_step = len(trainset)\n",
    "for epoch in range(param[2]):\n",
    "    for i, data in enumerate(trainset):\n",
    "        images, labels = data\n",
    "        imgs1=images[labels==1]\n",
    "        imgs3=images[labels==3]\n",
    "        imgs5=images[labels==5]\n",
    "        imgs9=images[labels==9]\n",
    "        imgs1=torch.cat([imgs1, imgs3], 0)\n",
    "        imgs1=torch.cat([imgs1, imgs5], 0)\n",
    "        imgs1=torch.cat([imgs1, imgs9], 0)\n",
    "#         tf.concat([imgs1, imgs5], 0)\n",
    "#         tf.concat([imgs1, imgs9], 0)\n",
    "        \n",
    "        lbl1=labels[labels==1]\n",
    "        lbl3=labels[labels==3]\n",
    "        lbl5=labels[labels==5]\n",
    "        lbl9=labels[labels==9]\n",
    "        lbl1=torch.cat([lbl1, lbl3], 0)\n",
    "        lbl1=torch.cat([lbl1, lbl5], 0)\n",
    "        lbl1=torch.cat([lbl1, lbl9], 0)\n",
    "#         tf.concat([lbl1, lbl3], 0)\n",
    "#         tf.concat([lbl1, lbl5], 0)\n",
    "#         tf.concat([lbl1, lbl9], 0)\n",
    "    \n",
    "        outputs = model(imgs1)\n",
    "        print(np.shape(outputs),np.shape(lbl1))\n",
    "        loss = criterion(outputs, lbl1)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, param[2], i+1, total_step, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.25\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    accurate=0\n",
    "    total=0\n",
    "    for images, labels in testset:\n",
    "        imgs1=images[labels==1]\n",
    "        imgs3=images[labels==3]\n",
    "        imgs5=images[labels==5]\n",
    "        imgs9=images[labels==9]\n",
    "        imgs1=torch.cat([imgs1, imgs3], 0)\n",
    "        imgs1=torch.cat([imgs1, imgs5], 0)\n",
    "        imgs1=torch.cat([imgs1, imgs9], 0)\n",
    "#         tf.concat([imgs1, imgs3], 0)\n",
    "#         tf.concat([imgs1, imgs5], 0)\n",
    "#         tf.concat([imgs1, imgs9], 0)\n",
    "        \n",
    "#         lbl1=labels[labels==1]\n",
    "#         lbl3=labels[labels==3]\n",
    "#         lbl5=labels[labels==5]\n",
    "#         lbl9=labels[labels==9]\n",
    "#         lbl1=torch.cat([lbl1, lbl3], 0)\n",
    "#         lbl1=torch.cat([lbl1, lbl5], 0)\n",
    "#         lbl1=torch.cat([lbl1, lbl9], 0)\n",
    "        \n",
    "        \n",
    "        lbl1=np.zeros(len(labels[labels==1]))\n",
    "        lbl3=np.zeros(len(labels[labels==3]))+1\n",
    "        lbl5=np.zeros(len(labels[labels==5]))+2\n",
    "        lbl9=np.zeros(len(labels[labels==9]))+3\n",
    "        lbl1=np.concatenate((lbl1, lbl3), axis=0)\n",
    "        lbl1=np.concatenate((lbl1, lbl5), axis=0)\n",
    "        lbl1=np.concatenate((lbl1, lbl9), axis=0)\n",
    "        \n",
    "        lbl1 = torch.tensor(lbl1.astype(np.long))\n",
    "        \n",
    "        \n",
    "#         tf.concat([lbl1, lbl3], 0)\n",
    "#         tf.concat([lbl1, lbl5], 0)\n",
    "#         tf.concat([lbl1, lbl9], 0)\n",
    "        output=model(imgs1)\n",
    "        _, predicted=torch.max(output.data,1)\n",
    "        p=predicted.numpy()\n",
    "        total=total+lbl1.size(0)\n",
    "        accurate=accurate+(predicted==lbl1).sum().item()\n",
    "#         print(accurate)\n",
    "    print(\"Accuracy :\",accurate/total)\n",
    "\n",
    "torch.save(model.state_dict(),\"Model.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
